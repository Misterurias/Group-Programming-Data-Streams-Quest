#!/usr/bin/env python3
"""
Streaming Analytics Module for Air Quality Data

This module provides real-time analytics capabilities by reading from
the CSV file generated by Phase 1 consumer and maintaining live statistics.

Author: Santiago BolaÃ±os Vega
Date: 2025-09-22
"""

import pandas as pd
import numpy as np
from collections import deque
from typing import Dict, List, Any, Optional, Tuple
import logging
from datetime import datetime, timedelta
import os

class StreamingAnalytics:
    """
    Real-time analytics engine that processes air quality data from CSV files.
    
    This class maintains sliding windows of recent data and calculates
    live statistics, correlations, and anomaly detection.
    """
    
    def __init__(self, csv_file: str, window_sizes: Dict[str, int] = None):
        """
        Initialize the streaming analytics engine.
        
        Args:
            csv_file: Path to the CSV file generated by Phase 1 consumer
            window_sizes: Dictionary defining window sizes for different time periods
        """
        self.csv_file = csv_file
        self.logger = logging.getLogger(__name__)
        
        # Default window sizes (in number of records)
        self.window_sizes = window_sizes or {
            'hourly': 24,      # Last 24 hours
            'daily': 168,      # Last 7 days (24*7)
            'weekly': 720,     # Last 30 days (24*30)
            'monthly': 2160    # Last 90 days (24*90)
        }
        
        # Initialize sliding windows
        self.windows = {
            'hourly': deque(maxlen=self.window_sizes['hourly']),
            'daily': deque(maxlen=self.window_sizes['daily']),
            'weekly': deque(maxlen=self.window_sizes['weekly']),
            'monthly': deque(maxlen=self.window_sizes['monthly'])
        }
        
        # Track last processed row
        self.last_processed_rows = 0
        
        # Live statistics
        self.current_stats = {
            'total_records': 0,
            'last_update': None,
            'pollutants': {
                'co_gt': {'mean': 0, 'std': 0, 'min': 0, 'max': 0},
                'nox_gt': {'mean': 0, 'std': 0, 'min': 0, 'max': 0},
                'no2_gt': {'mean': 0, 'std': 0, 'min': 0, 'max': 0},
                'c6h6_gt': {'mean': 0, 'std': 0, 'min': 0, 'max': 0}
            },
            'correlations': {},
            'anomalies': [],
            'temporal_patterns': {}
        }
        
        self.logger.info("Streaming analytics initialized")
    
    def get_new_data(self) -> pd.DataFrame:
        """
        Read new data from CSV file since last check.
        
        Returns:
            DataFrame with new records
        """
        try:
            if not os.path.exists(self.csv_file):
                self.logger.warning(f"CSV file {self.csv_file} not found")
                return pd.DataFrame()
            
            # Read the entire file
            df = pd.read_csv(self.csv_file)
            
            # Get only new rows
            if len(df) > self.last_processed_rows:
                new_data = df.iloc[self.last_processed_rows:]
                self.last_processed_rows = len(df)
                return new_data
            else:
                return pd.DataFrame()
                
        except Exception as e:
            self.logger.error(f"Error reading CSV file: {e}")
            return pd.DataFrame()
    
    def update_windows(self, new_data: pd.DataFrame) -> None:
        """
        Update sliding windows with new data.
        
        Args:
            new_data: DataFrame with new records
        """
        if new_data.empty:
            return
        
        # Convert to records and add to all windows
        records = new_data.to_dict('records')
        
        for window_name, window in self.windows.items():
            for record in records:
                window.append(record)
        
        self.logger.debug(f"Updated windows with {len(records)} new records")
    
    def calculate_pollutant_stats(self) -> Dict[str, Dict[str, float]]:
        """
        Calculate statistics for each pollutant across all windows.
        
        Returns:
            Dictionary with statistics for each pollutant
        """
        pollutant_columns = ['co_gt', 'nox_gt', 'no2_gt', 'c6h6_gt']
        stats = {}
        
        for window_name, window in self.windows.items():
            if len(window) == 0:
                continue
                
            window_df = pd.DataFrame(list(window))
            window_stats = {}
            
            for pollutant in pollutant_columns:
                if pollutant in window_df.columns:
                    values = pd.to_numeric(window_df[pollutant], errors='coerce')
                    values = values.dropna()
                    
                    if len(values) > 0:
                        window_stats[pollutant] = {
                            'mean': float(values.mean()),
                            'std': float(values.std()),
                            'min': float(values.min()),
                            'max': float(values.max()),
                            'count': len(values)
                        }
            
            stats[window_name] = window_stats
        
        return stats
    
    def calculate_correlations(self) -> Dict[str, Dict[str, float]]:
        """
        Calculate correlations between pollutants.
        
        Returns:
            Dictionary with correlation matrices for each window
        """
        pollutant_columns = ['co_gt', 'nox_gt', 'no2_gt', 'c6h6_gt']
        correlations = {}
        
        for window_name, window in self.windows.items():
            if len(window) < 2:
                continue
                
            window_df = pd.DataFrame(list(window))
            
            # Convert to numeric and drop NaN values
            numeric_df = window_df[pollutant_columns].apply(
                pd.to_numeric, errors='coerce'
            ).dropna()
            
            if len(numeric_df) > 1:
                corr_matrix = numeric_df.corr()
                correlations[window_name] = corr_matrix.to_dict()
        
        return correlations
    
    def detect_anomalies(self, threshold: float = 3.0) -> List[Dict[str, Any]]:
        """
        Detect statistical anomalies using Z-score method.
        
        Args:
            threshold: Z-score threshold for anomaly detection
            
        Returns:
            List of anomaly records
        """
        anomalies = []
        pollutant_columns = ['co_gt', 'nox_gt', 'no2_gt', 'c6h6_gt']
        
        # Use hourly window for anomaly detection
        if len(self.windows['hourly']) < 10:
            return anomalies
        
        window_df = pd.DataFrame(list(self.windows['hourly']))
        
        for pollutant in pollutant_columns:
            if pollutant not in window_df.columns:
                continue
                
            values = pd.to_numeric(window_df[pollutant], errors='coerce').dropna()
            
            if len(values) < 10:
                continue
            
            # Calculate Z-scores
            mean_val = values.mean()
            std_val = values.std()
            
            if std_val == 0:
                continue
            
            z_scores = np.abs((values - mean_val) / std_val)
            
            # Find anomalies
            anomaly_mask = z_scores > threshold
            anomaly_indices = values[anomaly_mask].index
            
            for idx in anomaly_indices:
                if idx < len(window_df) and idx in values.index:
                    record = window_df.iloc[idx].to_dict()
                    try:
                        anomaly_value = float(values.loc[idx])
                        anomaly_z_score = float(z_scores.loc[idx])
                        anomalies.append({
                            'pollutant': pollutant,
                            'value': anomaly_value,
                            'z_score': anomaly_z_score,
                            'timestamp': record.get('timestamp', 'unknown'),
                            'detected_at': datetime.now().isoformat()
                        })
                    except (IndexError, KeyError):
                        # Skip this anomaly if we can't access the values
                        continue
        
        return anomalies
    
    def analyze_temporal_patterns(self) -> Dict[str, Any]:
        """
        Analyze temporal patterns in the data.
        
        Returns:
            Dictionary with temporal pattern analysis
        """
        if len(self.windows['daily']) < 24:
            return {}
        
        window_df = pd.DataFrame(list(self.windows['daily']))
        
        # Convert timestamp to datetime with graceful handling
        if 'timestamp' in window_df.columns:
            try:
                # Try to parse timestamps, handling mixed formats gracefully
                window_df['datetime'] = pd.to_datetime(window_df['timestamp'], errors='coerce')
                
                # Only proceed if we have valid timestamps
                valid_timestamps = window_df['datetime'].notna()
                if valid_timestamps.sum() > 0:
                    window_df.loc[valid_timestamps, 'hour'] = window_df.loc[valid_timestamps, 'datetime'].dt.hour
                    window_df.loc[valid_timestamps, 'day_of_week'] = window_df.loc[valid_timestamps, 'datetime'].dt.dayofweek
                else:
                    self.logger.warning("No valid timestamps found for temporal analysis")
                    return {}
                    
            except Exception as e:
                self.logger.warning(f"Could not parse timestamps: {e}")
                return {}
        
        patterns = {}
        
        # Define pollutant columns to analyze
        pollutant_columns = ['co_gt', 'nox_gt', 'no2_gt', 'c6h6_gt']
        
        # Hourly patterns for all pollutants
        if 'hour' in window_df.columns:
            for pollutant in pollutant_columns:
                if pollutant in window_df.columns:
                    hourly_means = window_df.groupby('hour')[pollutant].mean()
                    patterns[f'hourly_{pollutant}_pattern'] = hourly_means.to_dict()
        
        # Day of week patterns for all pollutants
        if 'day_of_week' in window_df.columns:
            for pollutant in pollutant_columns:
                if pollutant in window_df.columns:
                    daily_means = window_df.groupby('day_of_week')[pollutant].mean()
                    patterns[f'daily_{pollutant}_pattern'] = daily_means.to_dict()
        
        return patterns
    
    def process_new_data(self) -> Dict[str, Any]:
        """
        Main processing function that updates all analytics.
        
        Returns:
            Dictionary with current analytics results
        """
        # Get new data
        new_data = self.get_new_data()
        
        if new_data.empty:
            return self.current_stats
        
        # Update windows
        self.update_windows(new_data)
        
        # Calculate statistics
        self.current_stats['pollutants'] = self.calculate_pollutant_stats()
        self.current_stats['correlations'] = self.calculate_correlations()
        self.current_stats['anomalies'] = self.detect_anomalies()
        self.current_stats['temporal_patterns'] = self.analyze_temporal_patterns()
        
        # Update metadata
        self.current_stats['total_records'] = self.last_processed_rows
        self.current_stats['last_update'] = datetime.now().isoformat()
        
        self.logger.info(f"Processed {len(new_data)} new records. Total: {self.last_processed_rows}")
        
        return self.current_stats
    
    def get_summary_stats(self) -> Dict[str, Any]:
        """
        Get summary statistics for dashboard display.
        
        Returns:
            Dictionary with summary statistics
        """
        summary = {
            'total_records': self.current_stats['total_records'],
            'last_update': self.current_stats['last_update'],
            'window_sizes': {name: len(window) for name, window in self.windows.items()},
            'recent_anomalies': len(self.current_stats['anomalies']),
            'data_file': self.csv_file,
            'file_exists': os.path.exists(self.csv_file)
        }
        
        return summary
